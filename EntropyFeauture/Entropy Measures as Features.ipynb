{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy Measures as Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximate Entropy\n",
    "\n",
    "Introduced in [RICHMAN2000PHYSIOLOGICAL]\n",
    "\n",
    "Approximate entropy is, more or less, equal to the natural logarithm of the conditional probability that, given two sequences of length $m$ are similar (within a distance $r$), what is the probability that they remain similar for length $m + 1$. Simply put, approximate entropy captures the changeability of a time series and is recognised as a measure of complexity and regularity of a time series.\n",
    "\n",
    "\n",
    "For a time series of fixed length N, fixed $m$ represents the length of the vectors to be compared and fixed $r$ represents the degree within which two vectors are considered similar. \n",
    "\n",
    "$$ ApEn(m,r,N) = \\lim_{N \\rightarrow \\infty} \\phi^{m}(r) - \\phi^{m+1}(r) $$\n",
    "\n",
    "Where, \n",
    "\n",
    "$$\\phi^{m}(r) = \\frac{1}{N-m+1}\\Sigma_{i=1}^{N-m+1} ln [C^{m}_{i}(r)]$$\n",
    "\n",
    "Where $C^{m}_{i}(r)$ is the probability that any vector $x_{m}(j)$ is within $r$ of $x_{m}(i)$\n",
    "\n",
    "$$  C^{m}_{i}(r) = \\frac{B_{i}}{N-m+1} $$\n",
    "\n",
    "and \n",
    "\n",
    "$$  C^{m+1}_{i}(r) = \\frac{A_{i}}{N-m+1} $$\n",
    "\n",
    "Where $B_{i} $ is the number of vectors within $r$ of $x_{m}(i)$ and $A_{i} $ is the number of vectors within $r$ of $x_{m+1}(i)$\n",
    "\n",
    "### Notes on Approximate Entropy\n",
    "\n",
    "<ul>\n",
    "    <li> When computing $C^{m}_{i}(r)$, how many vectors are within $r$ of vector $x_{m}(i)$, approxmiate entropy self-matches, or considers itself to be within $r$ of itself. This is to avoid division by zero in the equation,\n",
    "        $$ApEn(m,r) = \\lim_{N \\rightarrow \\infty} \\phi^{m}(r) - \\phi^{m+1}(r)$$\n",
    "This feature of the algorithm introduces a bias, trivial in the limitebut most obvious for finite series where $B_{i} = A_{i} = 0$ as these are assigned a conditional probability of 1, correspinding to perfect regularity or 0 entropy. Sample entropy seeks to address this bias.</li>\n",
    "    </ul>\n",
    "\n",
    "## Sample Entropy\n",
    "\n",
    "Sample Entropy was introduced in [RICHMAN2000PHYSIOLOGICAL] as a solution to the bias encountered in Approximate entropy. Sample entropy does not count self-matches when comparing vectors. Secodnly, Sample entropy is not template based. I.e. it does not find all templates first and then compare. Sample entropy only requires that one template find a vector of length $m+1$ lie within $r$. Therefore, where Aprroximate entropy calculates the conditional probability in a template way, Sample entropy computes the probability associated with the time series as a whole such that \n",
    "\n",
    "$$ SampEn(m,r) = \\lim_{N \\rightarrow \\infty} -ln[\\frac{A^{m}(r)}{B^{m}(r)}] $$\n",
    "\n",
    "where $B^{m}_{i}(r)$ is the number of  $m$ vectors within $r$ of $i$ and $A^{m}_{i}(r)$ is the number of $m + 1$ vectors within $r$ of $i$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Notes on Sample Entropy \n",
    "\n",
    "[RICHMAN2000PHYSIOLOGICAL] noted that Sample entropy maintained relative consistency over different value pairs of $(m,r)$ where Approximate entropy did not. However, Sample entropy suffers from residual bias that [RICHMAN2000PHYSIOLOGICAL] attribute to non indepdendence of templates. This is something we will revisit <b> can we exrtract independent templates from time series? </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permutation Entropy\n",
    "\n",
    "Introduced in [BANDT2002PERMUTATION], Permutation entropy computes probabilities over all possible permutations of vectors of varying length from a time series. \n",
    "\n",
    "For a time series $\\{x_{t}\\}_{t=1...T}$, Perm entropy studies all $n!$ permutations $\\Pi$ of ordern $n$. For each $\\Pi$, Perm entropy considers the relative frequencies of all possible permutations of that order such that:\n",
    "\n",
    "$$ p(\\Pi) = \\frac{\\# t|t \\leq T - n, (x_{t+1},...,x_{t-n}) of type \\Pi}{T-n+1} $$\n",
    "\n",
    "The overall Permutation entropy of the time series of length $n$ is therefore defined as, \n",
    "\n",
    "$$H(n) = - \\Sigma p(\\pi) log p(\\pi)$$\n",
    "\n",
    "### Notes on Permutation  Entropy\n",
    "\n",
    "Permutation entropy's strength comes from its ability to deal with noisy time series,, its efficiency and its ability to be applied to time series of any length. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singular Value Decomposition (SVD) Entropy\n",
    "\n",
    "Singular Value Decomposition performs dimensionality reduction on a matrix by finding the eigenvalues and vectors such that the original matrix $A_{np}$ can be re-written as,\n",
    "\n",
    "$$ A_{np} = U_{nn} S_{np} V^{T}_{pp}$$\n",
    "\n",
    "where \n",
    "\n",
    "<ul>\n",
    "    <li> $U_{nn}$ represents the left singular vectors </li>\n",
    "    <li> $S_{np}$ representes the singular values </li>\n",
    "    <li> $V^{T}_{pp}$ represents the right singular vectors </li>\n",
    "</ul>\n",
    "The SVD entropy of a time series with an embedded matrix with vector size of fixed $m$ and $M$ singular values can be calculated as\n",
    "\n",
    "$$ H = - \\Sigma_{i=1}^{m} \\sigma_{i} log_{2}(\\sigma_{i}) $$\n",
    "\n",
    "where,\n",
    "$$\\sigma = S_{np}$$\n",
    "\n",
    "### Notes on SVD Entropy\n",
    "\n",
    "Simply put, the SVD is a measure of the variability between the principal components of a time series. However, the matrix decomposition does not make use of the temporal direction of the time series. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropic Measures as Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now apply all the above described entropy measures to real world data. We want to see if, by using the above measures as aggregate features, we can see relationships between the structure of the time series and the most succesful measure as an aggregate feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Datasets\n",
    "\n",
    "<ul>\n",
    "    <li> Epilepsy dataset from [ANDRZEJAK2001INDICATIONS] </li>\n",
    "    <li> Depresjon dataset from [GARCIA-VEJA2018MOTOR] </li>\n",
    "    <li> Worms dataset from URL: http://www.timeseriesclassification.com/description.php?Dataset=Worms</li>\n",
    "    <li> Plane dataset from URL: https://timeseriesclassification.com/description.php?Dataset=Plane</li>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "<b> Experiment Set Up: </b>\n",
    "\n",
    "Each entropy measure (approximate entropy, sample entropy, pemrutation entropy and svd entopy] were calculated and added as an aggregated feature to the mean and standard deviation of the time series for each subject in the respective datasets. A Naive Bayes classifier was then trained and its classifcation prediction and F score was then evaluated through 10-cross-fold-validation. The results are discussed below.\n",
    "\n",
    "<b> Results: </b>\n",
    "\n",
    "|           | Approximate Entropy | Sample Entropy | Permutation Entropy | SVD Entropy | Number of Classes |\n",
    "|-----------|---------------------|----------------|---------------------|-------------|-------------------|\n",
    "| Epilepsy  | 0.578               | 0.578          | 0.516               | 0.638       | 5                 |\n",
    "| Depresjon | 0.747               | 0.727          | 0.73                | 0.65        | 2                 |\n",
    "| Planes    | 0.47                | 0.54           | 0.5                 | 0.68        | 7                 |\n",
    "| Worms     | 0.5                 | 0.45           | 0.39                | 0.49        | 5        \n",
    "\n",
    "\n",
    "<b> Discussion </b>\n",
    "\n",
    "The resuls show that the optimum entropy measure is variable over datasets. We now investigate whether the stationarity, autocorrelation of the datasets can be linked with which entropic feature is most predictively powerful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing for Stationarity\n",
    "\n",
    "The stationarity of a time series refers to longituidnal data whose attributes including mean and variance stay constant over time, the equivalent of a linear function with a constant slope. Stationary time series are a subset of all possible time series in nature and are relatively easy to model and understand. \n",
    "\n",
    "### Augmented Dickey-Fuller (ADF) Test\n",
    "\n",
    "The ADF test is an example of a unit root test, a group of statistical approaches to assess whether a time series contains a stochastic process, or a unit root, that makes the time series unpredictable and shift over time. \n",
    "\n",
    "The ADF tests the null hypothesis that $\\alpha = 1$ in the following equation,\n",
    "\n",
    "$$ y_{t} = C + \\beta t + \\alpha y_{t-1} + \\phi_{1} \\delta Y_{t-1} + \\phi_{2} \\delta Y_{t-2} ... + \\phi_{p} \\delta Y_{t-p} + \\epsilon_{t} $$\n",
    "\n",
    "If $\\alpha = 1$, then $y_{t}$ is composed of a unit root which means the time series shifts over time and is <b> non stationary </b>.\n",
    "\n",
    "If $\\alpha < 1$ , then the null hypothesis is rejected and the time series is considered <b> stationary </b>. \n",
    "\n",
    "NOTE: It's important to make sure that the p-value of the ADF test is less than the significance level in order to correctly reject the null hypothsis. In our case, we choose the signiifcance level to be 0.05 (time series is deemed stationary  5% of the time when it is actually non-stationary). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing for Autocorrelation\n",
    "\n",
    "The autocorrelation of a time series is measures by comparing values at a particular timestep with values from a previous tim step, often referred to as lags. \n",
    "\n",
    "\n",
    "### Durbin-Watson Statistic\n",
    "\n",
    "The Durbin Watson tests for the autocorrelation of a time series using the residuals from a least squares regression and checks for independence between these residuals.\n",
    "\n",
    "In least squares regression, the residuals represent the error between the linear estimators prediction of the target variable and the ground truth of the target label\n",
    "\n",
    "$$ dw = \\frac{\\Sigma_{i=2}^{n}(e_{i} - e_{i-1})^{2}}{\\Sigma_{i=1}^{n}e_{i}^{2}} $$\n",
    "\n",
    "where $e_{i}$ is the predicition error for value $y_{i}$ of the time series. \n",
    "\n",
    "By examining how the prediction errors of different values in the time series change over time we can assess whether or not there is some hidden pattern in the time series.\n",
    "\n",
    "$dw$ will always lie between 0 and 4 where $dw = 2$ shows no autocorrelation, $dw < 2$ shows positive autocorrelation and $dw > 2$ shows negative autocorrelation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Results\n",
    "\n",
    "\n",
    "|           | ADFuller statistic (mean)                                              | Durbin-Watson statistic (mean) |   |\n",
    "|-----------|------------------------------------------------------------------------|--------------------------------|---|\n",
    "| Epilepsy  | ADF: -8.68 rejected 100% Null | 0.11                           |   |\n",
    "| Depresjon | ADF:-12.67 rejected 100% Null  | 0.46                           |   |\n",
    "| Planes    | ADF: -2.88 rejected 50% Null  | 0.03                           |   |\n",
    "| Worms     | ADF: -4.26 rejected 80% Null  | 0.004                          |   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above results and related autocorrelation plots in the associated Jupyter notebooks show that there is significant autocorrelation in individual timeseries of the datasets\n",
    "\n",
    "Furthermore, sample entropy notes problems problems with template approach as it does not take into account the dependency with templates. SVD entropy takes the alternative approach of measuring the entropy over the time series decomposed into its principal components. However, SVD entropy then loses the temporal structure of the series.\n",
    "\n",
    "An interesting avenue of research would be to investigate a new entropy meausure that breaks down the time series into a representation that captures recurring patterns, or motifs, and anomalous components, or discords.\n",
    "\n",
    "### Entropy of a matrix profile as a feature\n",
    "\n",
    "We begin investigating this idea by utilising the STUMPY API and framework from [LAW2019STUMPY]. The STUMPY package is available for calculating the matrix profile of a time series. \n",
    "\n",
    "The Matrix profile of a time series takes as input a window size $m$ and breaks down a time series into all the possible subsequences of length $m$. It then computes the Z norm Euclidean distance between every subsequence and stores each subsequence's nearest neighbour and the associated distance. \n",
    "\n",
    "The idea here is that if a subsequence has its nearest neighbour closeby, it may represent a repeating pattern (or motif). Similarly, if a subsequence has its nearest neighbour faraway, it may represent an anomolous pattern (or discord)\n",
    "\n",
    "\n",
    "For our entropy measure we propose two <b> initial </b> ideas:\n",
    "<ul>\n",
    "<li>1: Calculate the entropy over the top k distances (top k motifs) and bottom k distances (top k discords) </li>\n",
    "    <li> 2: Calculate the entropy over the probability that a subsequence is part of a top k motif, or a top k discord </li>\n",
    "    </ul>\n",
    "    \n",
    "The results are shown in the associated Jupyter notebooks for each dataset. Our initial results that our proposed matrix profile entropy measures show promise at separating time serie from a range of datasets.\n",
    "\n",
    "### Future Work\n",
    "\n",
    "Future work includes analysing the shape of the time series and how they relate to the success of all entropy measures as features, including our proposed matrix profile entropy measures. We will also continue with our work investigating entropy of the matrix profile, hoping to achieve a more sophisticated feature with better predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li> [RICHMAN2000PHYSIOLOGICAL] Richman, Joshua S., and J. Randall Moorman. \"Physiological time-series analysis using approximate entropy and sample entropy.\" American Journal of Physiology-Heart and Circulatory Physiology 278.6 (2000): H2039-H2049. </li>\n",
    "    <li>[BANDT2002PERMUTATION] Bandt, Christoph, and Bernd Pompe. “Permutation entropy: a natural complexity measure for time series.” Physical review letters 88.17 (2002): 174102.</li>\n",
    "    <li> [ANDRZEJAK2001INDICATIONS] Andrzejak RG, Lehnertz K, Rieke C, Mormann F, David P, Elger CE (2001) Indications of nonlinear deterministic and finite dimensional structures in time series of brain electrical activity: Dependence on recording region and brain state, Phys. Rev. E, 64, 061907 </li>\n",
    "    <li> [GARCIA-VEJA2018MOTOR] Enrique Garcia-Ceja, Michael Riegler, Petter Jakobsen, Jim Tørresen, Tine Nordgreen, Ketil J. Oedegaard, Ole Bernt Fasmer, Depresjon: A Motor Activity Database of Depression Episodes in Unipolar and Bipolar Patients, In MMSys'18 Proceedings of the 9th ACM on Multimedia Systems Conference, Amsterdam, The Netherlands, June 12 - 15, 2018 </li>\n",
    "    <li> [LAW2019STUMPY] S.M. Law, (2019). STUMPY: A Powerful and Scalable Python Library for Time Series Data Mining. Journal of Open Source Software, 4(39), 1504. </li>\n",
    "    \n",
    "    \n",
    "   </ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
